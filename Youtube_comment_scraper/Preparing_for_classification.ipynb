{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the comments dataset\n",
    "comments_df = pd.read_csv(\"comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "2    151\n",
      "0     77\n",
      "1     72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"comment_sample.csv\")\n",
    "label_counts = df[\"Label\"].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sample Labeling for Testing\n",
    "\n",
    "In preparation for testing my fine-tuned DistilBERT model, I manually labeled a sample of 300 YouTube comments. The labeling process involved categorizing comments into three sentiment classes: 2 for neutral, 1 for positive, and 0 for negative. However, as the model was fine-tuned specifically for positive and negative sentiment analysis, I plan to remove the neutral class during evaluation to focus on the target sentiments.\n",
    "\n",
    "The positive and negative sentiment classes turned out to be randomly distributed, with approximately equal weighting. In the sample, there were 72 positive comments and 77 negative comments, while the remaining comments were labeled as neutral. This labeled sample will be used to assess the model's performance on YouTube comments, providing valuable insights into its ability to classify sentiments effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You guys are clueless about buyers agents. The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This guy sounds like ChatGpt wrote him üòÇ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>These woke, sorry ‚Äúred pill‚Äù takers, are going...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jcal is killing it lately lolü§£</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I love you JCAL!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comment  Label\n",
       "5   You guys are clueless about buyers agents. The...      0\n",
       "7            This guy sounds like ChatGpt wrote him üòÇ      0\n",
       "10  These woke, sorry ‚Äúred pill‚Äù takers, are going...      0\n",
       "12                     Jcal is killing it lately lolü§£      1\n",
       "15                                  I love you JCAL!!      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.drop(columns=\"Comment ID\")\n",
    "\n",
    "df_test = df_test[df_test[\"Label\"] != 2]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_test[\"Comment\"] = df_test[\"Comment\"].astype(str)\n",
    "\n",
    "# Preprocess the \"Comment\" column in one line:\n",
    "df_test[\"Comment\"] = df_test[\"Comment\"].str.lower().str.replace(r\"[^a-z0-9\\s]\", \"\", regex=True).str.strip().str.replace(r\"\\s+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Extracting the comments from the test dataset\n",
    "youtube_comments = df_test[\"Comment\"].tolist()\n",
    "\n",
    "# Tokenizing the YouTube comments using the same tokinzer\n",
    "tokenized_youtube_comments = tokenizer(\n",
    "    youtube_comments,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=103,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# Extracing input IDs and attenetion mask for the test dataset\n",
    "input_ids_test = tokenized_youtube_comments[\"input_ids\"]\n",
    "attention_mask_test = tokenized_youtube_comments[\"attention_mask\"]\n",
    "\n",
    "# Adding tokenized data to the original DataFrame for the test dataset\n",
    "df_test[\"input_ids\"] = input_ids_test.numpy().tolist()\n",
    "df_test[\"attention_mask\"] = attention_mask_test.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n",
    "\n",
    "df_test.to_csv(\"../Youtube_comment_scraper/df_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_ids_test = df_test[\"input_ids\"].tolist()\n",
    "attention_mask_test = df_test[\"attention_mask\"].tolist()\n",
    "\n",
    "# Converting lists to NumPy arrays for YouTube comment data\n",
    "input_ids_array = np.array(input_ids_test)\n",
    "attention_mask_array = np.array(attention_mask_test)\n",
    "\n",
    "# \"Label\" containts the ground truth for the YouTube comment data\n",
    "label_test = df_test[\"Label\"].values\n",
    "\n",
    "# Creating a dictionary for YouTube comment data\n",
    "test_data_np = {\n",
    "    \"input_ids\": input_ids_array,\n",
    "    \"attention_mask\": attention_mask_array\n",
    "}\n",
    "\n",
    "# Creating a TensorFlow dataset for the YouTube comment data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data_np, label_test))\n",
    "\n",
    "# Batching the YouTube comment data\n",
    "test_dataset = test_dataset.batch(batch_size=32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "\n",
    "model_path = \"../NLP_model/best_sentiment_model.keras\"\n",
    "\n",
    "# Loading the best model from local directory\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={\"TFDistilBertModel\": TFDistilBertModel}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 103)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 103)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDi  TFBaseModelOutput(last_hid   6636288   ['input_ids[0][0]',           \n",
      " stilBertModel)              den_state=(None, 103, 768)   0          'attention_mask[0][0]']      \n",
      "                             , hidden_states=None, atte                                           \n",
      "                             ntions=None)                                                         \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 768)                  0         ['tf_distil_bert_model[0][0]']\n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 768)                  0         ['global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['dropout_19[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66363649 (253.16 MB)\n",
      "Trainable params: 66363649 (253.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned DistilBERT model, specifically trained for binary sentiment classification, is then loaded, retaining its task-specific configuration. This enables the model to predict sentiment based on its training with similar data.\n",
    "\n",
    "In the subsequent sections, the loaded DistilBERT model is applied to predict sentiment in the prepared YouTube comments. The analysis aims to reveal insights into the model's effectiveness in discerning sentiment within the context of user-generated content on the YouTube platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0 (30522, 768)\n",
      "tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0 (512, 768)\n",
      "tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0 (768, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0 (768, 3072)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0 (3072,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0 (3072, 768)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0 (768,)\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0 (768,)\n",
      "dense/kernel:0 (768, 1)\n",
      "dense/bias:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "# Checking wheather the weights have been loaded correctly\n",
    "for layer in loaded_model.layers:\n",
    "    for weight in layer.weights:\n",
    "        print(weight.name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the YouTube dataset\n",
    "predictions_test = loaded_model.predict(test_dataset)\n",
    "\n",
    "# Print logits for debugging\n",
    "print(\"Logits for the YouTube dataset:\", predictions_test)\n",
    "\n",
    "# Converting logits to probabilities using softmax\n",
    "probabilities_labels = tf.nn.softmax(predictions_test, axis=-1)\n",
    "\n",
    "# Print probabilities for debugging\n",
    "print(\"Probabilities for YouTube dataset:\", probabilities_labels)\n",
    "\n",
    "# Getting the predicted label values\n",
    "predicted_labels = tf.argmax(probabilities_labels, axis=-1)\n",
    "\n",
    "print(\"Predicted Labels for YouTube comment dataset:\", predicted_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 - 5s - loss: 0.5232 - accuracy: 0.8658 - 5s/epoch - 1s/step\n",
      "Test Loss: 0.5232, Test Accuracy: 0.8658\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = loaded_model.evaluate(test_dataset, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 149 occurences\n"
     ]
    }
   ],
   "source": [
    "y, idx, count = tf.unique_with_counts(predicted_labels)\n",
    "\n",
    "for label, count in zip(y.numpy(), count.numpy()):\n",
    "    print(f\"Label {label}: {count} occurences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
